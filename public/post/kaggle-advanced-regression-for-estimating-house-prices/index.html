<!DOCTYPE HTML>
<html>

    <head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description" content="Your description">
	<meta name="author" content="Dan Bernstein">
	<meta name="generator" content="Hugo 0.40.3" />
	<title>Kaggle: Advanced Regression for Estimating House Prices &middot; Data By Dan</title>
	<!-- Stylesheets -->
	
	<link rel="stylesheet" href="https://danbernstein.netlify.com/css/main.css"/>
	
	

	

	<!-- Custom Fonts -->
	<link href="https://danbernstein.netlify.com/css/font-awesome.min.css" rel="stylesheet" type="text/css">

	
	<link rel="shortcut icon" type="image/x-icon" href="https://danbernstein.netlify.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://danbernstein.netlify.com/favicon.ico">
	

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="js/ie/html5shiv.js"></script>
	<script src="js/ie/html5shiv.jsrespond.min.js"></script>
	<![endif]-->
</head>
    <body>

    <!-- Wrapper -->
        <div id="wrapper">

            <!-- Header -->
    <header id="header" class="alt">
        <a href="https://danbernstein.netlify.com/" class="logo"><strong>Forty</strong> <span>By HTML5 Up</span></a>
        <nav>
            <a href="#menu">Menu</a>
        </nav>
    </header>

<!-- Menu -->
    <nav id="menu">
        <ul class="links">
            
                <li><a href="https://danbernstein.netlify.com/">Home</a></li>
            
                <li><a href="https://danbernstein.netlify.com/post/cv">Resume</a></li>
            

        </ul>
        <ul class="actions vertical">
            
            
        </ul>
    </nav>

        <!-- Main -->
            <div id="main" class="alt">

                
                    <section id="one">
                        <div class="inner">
                            <header class="major">
                                <h1>Kaggle: Advanced Regression for Estimating House Prices</h1>
                            </header>
                            
                            

<p>Summary: I looked to Kaggle to further practice building predictive models. After optimizing single and ensemble regression techniques, I uncovered ensemble stacking as a method for building a strong predictive model from a collection of weak learners. The outcome is drastic improvements in predictive accuracy. This post will provide an overview of:</p>

<ol>
<li>the basics of automating data preparation using <a href="http://topepo.github.io/caret/index.html">caret</a></li>
<li>building stacked ensemble modelling using <a href="https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html">caretEnsemble</a></li>
<li>reason through how the various models that I used improve the ensemble predictions</li>
</ol>

<p>This post will deal less with the specifics of the dataset, and rather provide an overview of how these packages provide easy, flexible, and powerful methods for developing strong predictive models.</p>

<h2 id="using-caret-to-accelerate-data-processing-and-feature-selection">Using caret to accelerate data processing and feature selection</h2>

<p>The <a href="http://topepo.github.io/caret/index.html">caret</a> package is great for automating data pre-processing, feature selection, and tuning machine learning algorithms. It only takes one glance at the dimensions of the dataset in <a href="https://www.kaggle.com/c/santander-value-prediction-challenge">Kaggle&rsquo;s Santander Value Prediction Challenge</a>, which starts with over 4000 variables, to realize that tools are needed to quickly identify useless variables, especially those with little or no variance, and normalize highly skewed variables. caret can handle these repetitive actions through the preProcess function. Here we see that caret will evaluate all the variables for near-zero variance (&ldquo;nzv&rdquo;) and perform the Box Cox and Yeo-Johnson Power transformations to normalize skewed data.</p>

<pre><code class="language-r">
</code></pre>

<h2 id="improving-predictive-accuracy-with-stacked-ensemble-models">Improving predictive accuracy with stacked ensemble models</h2>

<h3 id="basics-of-tuning-a-model-with-caret">basics of tuning a model with caret</h3>

<p>The <a href="https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html">caretEnsemble</a> package streamlines the process of building and evaluating stacked ensemble models. You can tune the parameters of any model included in caret:</p>

<pre><code class="language-r">control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 1,
                        savePredictions = &quot;final&quot;, 
                        classProbs = F)

mod &lt;- caret::train(y_train~., 
                    data = train.df, # the trianing dataset 
                    trControl = control, #
                    method = 'glm')
mod
</code></pre>

<p>trainControl dictates how the model will be evaluated. Here we specify that the model perform repeated cross-validation (method = &ldquo;repeatedcv&rdquo;). meaning we will perform k-fold cross-validation multiple times. k-fold cross-validation involves splitting the data in k groups of roughly equal size, training the data repeatedly on all but one of these groups, and then testing the model on the one group that was left out. The test error from the k models is then averaged to provide a final test error to estimate the final model parameters.</p>

<h3 id="building-a-stacked-ensemble-model-with-caretensemble">building a stacked ensemble model with caretEnsemble</h3>

<h4 id="fundamental-concept-of-stacked-ensemble-models">Fundamental concept of stacked ensemble models</h4>

<p>Stacked ensemble models have proven quite successful in winning Kaggle competitions because they leverage the unique strengths of various ML algorithms to build a stronger model. The basic idea is that multiple models are built on the training data (depicted by the three y-hat objects in the figure below, likely a decision tree, SVM, and neural network here). The resulting predicted values from these models, known as the base learner models, are used as the input to a second-level algorithm, often called the meta-model. The meta-model will produce a model that optimizes the predicted values from the base learners, providing a single stronger model because each base learner will often pick up on different patterns in the data.
<img src="/img/blogs/modelstacking.png" style = "display: block;
    margin: 0 auto; background-color:white;";></p>

<h4 id="implementation-in-caretensemble">Implementation in caretEnsemble</h4>

<p>To implement a stacked ensemble model, we first have to bundle the trained base learners using caretList. Again, trControl determines how each model will be evaluated. It is important to specify the index for the base learners so that all the models train on the same data partitions. We specify the individual base learners in two ways. First, for any base learners that do not allow for parameter tuning, we provide them as a vector to the methodList parameter of caretList. For variables that allow for parameter tuning in caret, we will list the models individually as a list for &ldquo;tuneList&rdquo;. For each model, we first tune the parameters like we saw above, and then specify the best set of parameters as a dataframe in the tuneGrid parameter of the caretModelSpec function. By specifying single values for the model parameters, we are speeding up computation because caretEnsemble will not attempt to optimize the models.</p>

<pre><code class="language-r">
# these three lines set up parallel processing, which will speed up computation
library(doParallel)
cl &lt;- makePSOCKcluster(2)
registerDoParallel(cl)

# the trainControl function
control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 1,
                        savePredictions = &quot;final&quot;, 
                        index = createFolds(train.df$y_train, 2),
                        classProbs = F)

algorithmList &lt;- c('glmboost', 'gbm', 'bayesglm') 
set.seed(1000)

# train the base learners
models &lt;- caretList(formula = y_train~., 
                    data = train.df,
                    trControl = control,
                    methodList = algorithmList,
                    tuneList = list(
                    xgbTree=caretModelSpec(method=&quot;xgbTree&quot;, 
                                        tuneGrid=data.frame(.nrounds = 100, 
                                                            .max_depth = 4, 
                                                            .eta = 0.1,
                                                            .gamma = 0,
                                                            .colsample_bytree = 0.3,
                                                            .min_child_weight = 0.5,
                                                            .subsample = 1))
                                   ))
</code></pre>

<p>For the meta-model, we again specify the how the model will be evaluated, here stored in the stackControl object. We specify the meta-model as the method to the caretStack function, here a general linear model (glm), and the best model will be selected based on root mean squared error (RMSE). The final model demonstrates a significant improvement in RMSE and adjusted R-squared.</p>

<pre><code class="language-r"># train the meta-model

stackControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5,
                             savePredictions = &quot;final&quot;, 
                             classProbs = F)

stack.glm &lt;- caretStack(out, method = 'glm', 
                        metric = &quot;RMSE&quot;, trControl = stackControl)

stack.glm
</code></pre>

<h2 id="understanding-the-advantages-of-specific-ml-algorithms">Understanding the advantages of specific ML algorithms</h2>

<p>When trying to improve the ensemble predictive accuracy, it is easy to throw in many base leaners that don&rsquo;t provide large improvements in accuracy. caretEnsemble tries to warn</p>

                        </div>
                    </section>
            </div>
            
        <!-- Footer -->
            
                <!-- Footer -->
    <footer id="footer">
        <div class="inner">
            <ul class="icons">
                
                    <li><a href="https://www.facebook.com/dan.bernstein.771?ref=bookmarks" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
                
                    <li><a href="https://www.instagram.com/dan_bernstein88/" class="icon alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
                
                    <li><a href="https://github.com/danbernstein?tab=repositories" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                
                    <li><a href="https://www.linkedin.com/in/danbernstein94/" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
                
            </ul>
            <ul class="copyright">
                
                <li>Design:  <a href="https://www.html5up.net">HTML5 UP</a></li>
                
            </ul>
        </div>
    </footer>

            
        </div>

    <!-- Scripts -->
        <!-- Scripts -->
    <!-- jQuery -->
    <script src="https://danbernstein.netlify.com/js/jquery.min.js"></script>
    <script src="https://danbernstein.netlify.com/js/jquery.scrolly.min.js"></script>
    <script src="https://danbernstein.netlify.com/js/jquery.scrollex.min.js"></script>
    <script src="https://danbernstein.netlify.com/js/skel.min.js"></script>
    <script src="https://danbernstein.netlify.com/js/util.js"></script>

    

    <!-- Main JS -->
    <script src="https://danbernstein.netlify.com/js/main.js"></script>

    

    
        

    </body>
</html>
