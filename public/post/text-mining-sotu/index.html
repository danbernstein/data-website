<!DOCTYPE HTML>
<html>

    <head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description" content="Your description">
	<meta name="author" content="Dan Bernstein">
	<meta name="generator" content="Hugo 0.34" />
	<title>Text Mining &middot; Forty</title>
	<!-- Stylesheets -->
	
	<link rel="stylesheet" href="/css/main.css"/>
	
	

	

	<!-- Custom Fonts -->
	<link href="/css/font-awesome.min.css" rel="stylesheet" type="text/css">

	
	<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
	<link rel="icon" type="image/x-icon" href="/favicon.ico">
	

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="js/ie/html5shiv.js"></script>
	<script src="js/ie/html5shiv.jsrespond.min.js"></script>
	<![endif]-->
</head>
    <body>

    <!-- Wrapper -->
        <div id="wrapper">

            <!-- Header -->
    <header id="header" class="alt">
        <a href="/" class="logo"><strong>Forty</strong> <span>By HTML5 Up</span></a>
        <nav>
            <a href="#menu">Menu</a>
        </nav>
    </header>

<!-- Menu -->
    <nav id="menu">
        <ul class="links">
            
                <li><a href="/">Home</a></li>
            
                <li><a href="/blogs">Blogs</a></li>
            
                <li><a href="/blogs/ipsum">Generic Blog</a></li>
            
                <li><a href="/elements.html">Elements</a></li>
            

        </ul>
        <ul class="actions vertical">
            
                <li><a href="/blogs" class="button special fit">Get Started</a></li>
            
            
                <li><a href="/" class="button fit">Log In</a></li>
            
        </ul>
    </nav>

        <!-- Main -->
            <div id="main" class="alt">

                
                    <section id="one">
                        <div class="inner">
                            <header class="major">
                                <h1>Text Mining</h1>
                            </header>
                            
                            

<p>I&rsquo;ve wanted to explore text mining for awhile. The idea of drawing out core concepts and subtle themes among a corpus of documents adds a quantitative angle to analysis, whether in literature, policy, or science. This post is the first in a series using the <a href="https://www.tidytextmining.com/tidytext.html">tidytext</a> library and the great free online book, <a href="https://www.tidytextmining.com/tidytext.html">Text Mining in R</a> to explore some interesting collections of text.</p>

<h2 id="the-state-of-the-union"><strong>The State of the Union</strong></h2>

<p>Every president to date has delivered the State of the Union (SOTU) address annually to announce successes from the previous year and set administration priorities for the year to come. The enduring intent for the SOTU makes it a prime candidate for text mining, to illustrate how the language surrounding political messaging has changed over two hundred years. Using the tools available in the tidytext and <a href="https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf">topicmodels</a> packages, text mining reveals a number of trends.</p>

<div>
    <a href="https://plot.ly/~danbernstein/5/?share_key=v9uX92K4tJ912F9YS2EBhe" target="_blank" title="Plot 5" style="display: block; text-align: center;"><img src="https://plot.ly/~danbernstein/5.png?share_key=v9uX92K4tJ912F9YS2EBhe" alt="Plot 5" style="max-width: 100%;width: 700px;"  width="700" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
    <script data-plotly="danbernstein:5" sharekey-plotly="v9uX92K4tJ912F9YS2EBhe" src="https://plot.ly/embed.js" async></script>
</div>

<p>Topic Modeling:</p>

<p>The most apparent trend is the long-term changes in topic model compositions in each speech. Topic modelling is an unsupervised learning method that attempts to separate a collection of documents in distinct groups. I used the Latent Dirichlet allocation (LDA) readily availble in the topicmodels package to sort the 238 addresses into three classes (k = 3). LDA does allow for overlap in the use of terms in multiple topics, so the LDA outputs each document as a mixture of the classes.</p>

<p>The two major outputs of the topicmodels LDA approach are word-topic probabilities (beta values), which measure the tendency for a particular term to appear in one topic, with higher values demonstrating higher proclivity. The other value is document-topic probabilities (gamma values) which estimates the number of words in a document that are attributed to each topic.</p>

<p>In the chart above, we see a clear temporal component associated with the document-topic probabilities. The SOTU between 1790 and 1860 show little mixture in gamma values, indicating strong similarity in the language in the addresses. As the first topic class decreases, the second topic class increases, reaching its peak in 1908. It is worth noting that the third topic class shows little to no change as the other two classes fluctuate. The third topic class does spike at nearly 20% probability in Woodrow Wilson&rsquo;s 1917 address, further analysis might indicate why this speech was more similar to more recent addresses than other addresses from the time or from Wilson himself. Finally, from Nixon in 1970 onward, we see strong consistency of the third topic class, as the earlier document-topic probabilities drop to zero.</p>

<p>Total Frequency-Inverse Document Frequency:</p>

<p>While topic modelling can help us group documents into similar classes, there are other measures to identify the unique aspects of each address. Inverse Document Frequency (IDF) is a measure of term usage that increases the weight for terms that are used infrequently across a collection of documents and decreases words that appear commonly. When combined with the word&rsquo;s term frequency within each document, we can identify words that are unique to each year&rsquo;s address.</p>

<p>When you pan across the plot you will see the five terms with the highest TF-IDF value for each document, essentially creating a fingerprint for the address that distinguishes it from all others. Using this measure, we see words related to the War on Terror, such as &ldquo;terrorist&rdquo;, &ldquo;Iraq&rdquo;, and &ldquo;Saddam&rdquo; characterize all of George W. Bush&rsquo;s addresses, except the first address which was the only one to precede <sup>9</sup>&frasl;<sub>11</sub>.
While Ronald Reagan was the first president to invite acknowledged guests to attend the SOTU in 1982, we do not see the unique guest names until Reagan&rsquo;s 1985 address (&ldquo;Hale&rdquo; and &ldquo;Jean&rdquo;). It appears that Reagan&rsquo;s addresses before 1985 were dominated by references to spending and the deficit.</p>

<h2 id="document-simarility"><strong>Document Simarility</strong></h2>

<p>The <a href="https://cran.r-project.org/web/packages/quanteda/quanteda.pdf">quanteda</a> package provides a number of document similarity measures. This section outlines three common similarity measures that are implemented in the <a href="https://bikeshareviz.shinyapps.io/sotuviz_shiny/">shiny app</a>.</p>

<p><strong>Correlation Similarity</strong>
Correlation similarity is the pearson coefficient between the document terms. You can normalize the input document-feature matrix (dfm) to remove the bias towards longer documents by wrapping the input dfm in dfm_weight(x, &ldquo;prop&rdquo;).</p>

<p><strong>Jaccard Similarity:</strong>
The Jaccard Similarity Index interprets document similarity as the size of the intersection between two documents and the size of the union of the two.</p>

<p><img src="https://github.com/danbernstein/feb/blob/master/public/img/jaccard_eq.png" alt="Jaccard Equation" /></p>

<p>It is important to note that the Jaccard Index is bias towards longer documents, so documents with a higher occurrence of a given word will give greater weight to that word, regardless of document length. The Jaccard Index also does not discriminate between words that are common across the corpus and words that are unique to a select few. For that reason, it is common to apply a transformation on the raw term frequency counts, such as TF-IDF, before applying a similarity measure.</p>

<p><strong>Cosine Similarity:</strong>
Cosine Similarity takes in documents represented as vectors, through TF-IDF or another transformation, and compares directionality to determine the cosine angle between each pair. A cosine value of one indicates that the document are identical, while a value of zero indicates that the documents share no similarity. When using cosine similarity, all documents are normalized, removing the length bias seen in Jaccard Similarity. This transformation is inherent in the method, so you do not need to apply dfm_weight as you might when using correlation.</p>

<p><img src="https://github.com/danbernstein/feb/blob/master/public/img/cosinesimil_eq.png" alt="Cosine Equation" /></p>

<p><strong>Not All Similarity Measures Are Made the Same</strong>
Using raw term usage can yield misleading similarity measures. When you think about the most common words in the english language (i.e., &ldquo;and&rdquo;, &ldquo;is&rdquo;). To get a better measure of document similarity, we can first apply a transformation to weight words that are more unique in each document (a method called Total Frequency-Inverse Document Frequency, or TF-IDF), and then normalize coefficients to remove any bias towards larger values.</p>

<p>If we look back at the method for calculating each of the similarity measures, we see that they won&rsquo;t all work with the TF-IDF correction. Jaccard Similarity simply measures the fraction of words that are common to two documents. TF-IDF adds weight to unique words and lessens the weight of words common across the corpus, but it will not change the actual presence of words in the documents. When we compare the Jaccard measures for similarity matrices using TF vs. TF-IDF, we still there is little difference between the outputs.</p>

<p>Alternatively, both cosine and correlation similarity show much lower measures when we move from TF to TF-IDF data because the measures are based on more complex weighting, rather than the simple presence or absence of words.</p>

<pre><code class="language-r"></code></pre>

                        </div>
                    </section>
            </div>
            
        <!-- Footer -->
            
                <!-- Footer -->
    <footer id="footer">
        <div class="inner">
            <ul class="icons">
                
                    <li><a href="https://www.twitter.com" class="icon alt fa-twitter" target="_blank"><span class="label">Twitter</span></a></li>
                
                    <li><a href="https://www.facebook.com/dan.bernstein.771?ref=bookmarks" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
                
                    <li><a href="https://www.instagram.com/dan_bernstein88/" class="icon alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
                
                    <li><a href="https://github.com/danbernstein?tab=repositories" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                
                    <li><a href="https://www.linkedin.com/in/danbernstein94/" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
                
            </ul>
            <ul class="copyright">
                <li>&copy; Company Name</li>
                
                <li>Design:  <a href="https://www.html5up.net">HTML5 UP</a></li>
                
            </ul>
        </div>
    </footer>

            
        </div>

    <!-- Scripts -->
        <!-- Scripts -->
    <!-- jQuery -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>

    

    <!-- Main JS -->
    <script src="/js/main.js"></script>

    

    

    </body>
</html>
