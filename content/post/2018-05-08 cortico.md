---
title: Analyzing Psoriasis Dose Treatment Regimens 
author: Dan Bernstein
date: '2018-05-08'
slug: cortico
categories: []
tags:
  - R
  - data-wrangling
  - survey-analysis
description: ''
image: cortico.jpg
keywords: ''
draft: no
---

## My journey in data science

<iframe src="/thesismkdwn.pdf" style="float: right;" width="50%" height="600px">
This browser does not support PDFs. Please download the PDF to view it: <a href="/pdf/sample-3pp.pdf">Download PDF</a>
</iframe>

During my junior year of college, I took a class on Literature and Medicine. During the semester, I came across a pop science article about a new concept in medicine that leveraged the placebo effect in drug regimen. I explored the scientific foundations of this new paradigm through my midterm and final paper and argued for additional, ethic trials of Placebo-Controlled Dose Reduction (PCDR) in psoriasis treatment, where there has already been promising results. While I cannot do these trials, I became interested in estimating what this new treatment would mean for drug expenditure, which is a persistent issue in the U.S. healthcare system. 

I started searching for any literature estimating the individual or collective expenditure savings, and found only a few publications. Additionally, these studies were either outdated and did reflect the current scientific understanding concerning leveraging PCDR in corticosteroid treatment for managing psoriasis, or used unique and proprietary datasets. There was room for a updated estimate of the potential savings based on publically available data. This is where I started my study, with no knowledge of statistical computing or public datasets. This work was done as my senior thesis to complete my requirements for the University Honors Program at GW. 

## Public Data

From the literature review, I started looking for public datasets that would allow me to estimate annual spending on corticosteroids, ideally for psoriasis treatment. I began with the CDC's National Health and Nutrition Examination Survey (NHANES), which yields estimates of disease prevalence, with little information on expenditure. I moved on to the Department of Health and Human Services' Medical Expenditure Panel Survey (MEPS), which provided a rich dataset for analyzing prevalance and expenditure. 


## Statistical Computing

I had heard of SAS, STATA, and SPSS, all used by researchers for various purposes. I knew STATA was good for economics, SPSS for psychology, etc. I chose SAS as a starting point, learning from online tutorials to develop the fundamentals. After months of data wrangling and analysis, my old Macbook couldn't handle SAS any longer; running the analyses took a long time and I felt I needed to find a new way. 

Enter R. I had shied away from R and Python, they sounded a little too intimidating to me as someone with no programming experience. But, faced with SAS' frustrations, I began learning R. With online tutorials and the wonderful community online (RBloggers, Stack Overflow, etc.), I learned enough to transform the data into a form I wanted and utilize the packages required for complex survey analysis. The code was a terrible mashup of base R and the tidyverse. The results from this analysis were promising, and won me second place in the Politics & Economics division of GW's undergraduate research conference. Not bad for a chemistry major who started from nothing. 

I returned to this study a year after graduating to clean it up (almost entirely in the tidyverse this time), make it reproduceable, and generate a clean final product. Looking back, I can see the the evolution of my abilities, from thousands of lines of SAS code, to terribly inefficient R code (no vectorized functions, very little use of loops, etc.), to more efficient R code. The final analysis is not terribly complex, there is no modelling, and the results are not groundbreaking, but they are novel and they are unique. 


