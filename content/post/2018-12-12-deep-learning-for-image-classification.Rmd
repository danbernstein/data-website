---
title: Deep Learning for Image Classification
author: ''
date: '2018-12-12'
slug: deep-learning-for-image-classification
categories: []
tags: []
description: ''
image: ''
keywords: ''
draft: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      message= F, 
                      warning = F,
                      fig.align = "center")

```

At a meeting of the Transportation Techies Meet Up in early 2018, another Techie announced that he had gathered images from traffic cameras around Arlington, VA at intersections with bike lanes. The data is available at [here](http://parkingdirty.com/). He suggested that this data would likely be well-suited for computer vision (CV) to automate the process of classifying the images based on whether the bike lane was blocked or not. 

I recently decided to learn about computer vision to tackle this challenge. Recognizing that CV is not well-implemented in R, I read up on the major differences between R and Python, and then worked through the following texts:

* [Programming Computer Vision with Python](http://programmingcomputervision.com/downloads/ProgrammingComputerVision_CCdraft.pdf)

* [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)

While the first text gave a good conceptual understanding of computer vision, this task requires a more advanced analytical approach to annotate images and greater computational strength to power it. Another Techie has worked through this same problem using deep learning algorithms powered by Google's Colaboratory cloud environment to achieve good results. Using his work as a guide, I will develop similar algorithms with more of a focus on discussing how various techniques can be used to improve deep learning model results. You can see my work [here](https://colab.research.google.com/drive/1x15ai4nwfp0xxY1sfvX8UB0EHRcKXhU8#scrollTo=fPx3kopWDTIT). 

In working through this problem, I am guided by the "typical machine learning workflow" that Francois Chollet outlines in the second text: 

* First, define the problem: what data is available, and what are you trying to predict? Will you need to collect more data, to hire people to manually label a dataset?

* Identify a way to reliably measure success on your goal. For simple tasks, this may be prediction accuracy, but in many cases it will require sophisticated domain-specific metrics.

* Prepare the validation process that you will use to evaluate your models. In particular, you should define a training set, validation set, and test set. Your validation and test set labels should not "leak" into your training data: for instance, with temporal prediction, the validation and test data should be posterior to the training data.

* Vectorize your data, by turning it into vectors and preprocessing it in a way that makes it more easily approachable by a neural network (normalization, etc).

* Develop a first model that beats a trivial common-sense baselineâ€”thus demonstrating that machine learning can work on your problem. This might not always be the case!

* Gradually refine your model architecture by tuning hyperparameters and adding regularization. Make changes based on your performance on the validation data only, not the test data nor the training data. Remember that you should manage to get your model to overfit (thus identifying a model capacity level that is above that you need), and only then start adding regularization or start downsizing your model.

* Mind "validation set overfitting" when turning hyperparameters, i.e. the fact that your hyperparameters might end up being over-specialized to your validation set. Avoiding this is precisely the purpose of having a separate test set!
